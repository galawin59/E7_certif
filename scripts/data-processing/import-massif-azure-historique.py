#!/usr/bin/env python3
"""
E7 CERTIFICATION - IMPORTEUR MASSIF AZURE SQL HISTORIQUE
========================================================
Description: Import en masse des 651 fichiers d'historique FICP quotidien
             vers Azure SQL Database avec cramage intensif des cr√©dits !
Version: 1.0.0
Author: E7 Data Engineering Team - Expert FICP Cr√©dit Agricole
Date: 2025-10-30
License: MIT

STRATEGY CRAMAGE CR√âDITS:
- Import par BATCH de 1000 records pour optimiser throughput
- Parall√©lisation des imports par type de fichier
- Monitoring en temps r√©el de la consommation DTU
- 264,451 enregistrements √ó 3 types = 793,353 op√©rations SQL !
"""

import pandas as pd
import pyodbc
import os
from pathlib import Path
import logging
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

class ImporteurMassifAzureHistorique:
    """Importeur massif d'historique FICP vers Azure SQL Database"""
    
    def __init__(self):
        # Configuration Azure SQL Database
        self.server = 'sql-server-ficp-5647.database.windows.net'
        self.database = 'db-ficp-datawarehouse'
        self.username = 'ficpadmin'
        self.password = 'FicpDataWarehouse2025!'
        
        # Statistiques cramage
        self.total_records_imported = 0
        self.total_files_processed = 0
        self.start_time = None
        self.lock = threading.Lock()
        
    def get_connection_string(self):
        """Retourne la cha√Æne de connexion Azure SQL"""
        return (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"
            f"SERVER={self.server};"
            f"DATABASE={self.database};"
            f"UID={self.username};"
            f"PWD={self.password};"
            f"Encrypt=yes;"
            f"TrustServerCertificate=no;"
            f"Connection Timeout=30;"
        )
    
    def import_consultations_file(self, file_path):
        """Import d'un fichier de consultations vers Azure SQL"""
        try:
            df = pd.read_csv(file_path)
            if df.empty:
                return 0
            
            conn_str = self.get_connection_string()
            with pyodbc.connect(conn_str) as conn:
                cursor = conn.cursor()
                
                # Import par batch de 1000 pour performance
                batch_size = 1000
                records_imported = 0
                
                for i in range(0, len(df), batch_size):
                    batch = df.iloc[i:i+batch_size]
                    
                    # Pr√©parer les valeurs
                    values = []
                    for _, row in batch.iterrows():
                        values.append((
                            row['date_consultation'],
                            row['cle_bdf'],
                            row['reponse_registre'],
                            row['etablissement_demandeur'],
                            row['heure_consultation']
                        ))
                    
                    # Insert batch
                    cursor.executemany("""
                        INSERT INTO ConsultationsFICP 
                        (DateConsultation, CleBDF, ReponseRegistre, EtablissementDemandeur, HeureConsultation)
                        VALUES (?, ?, ?, ?, ?)
                    """, values)
                    
                    records_imported += len(batch)
                
                conn.commit()
                
                with self.lock:
                    self.total_records_imported += records_imported
                    self.total_files_processed += 1
                
                return records_imported
                
        except Exception as e:
            logger.error(f"‚ùå Erreur import consultations {file_path}: {e}")
            return 0
    
    def import_inscriptions_file(self, file_path):
        """Import d'un fichier d'inscriptions vers Azure SQL"""
        try:
            df = pd.read_csv(file_path)
            if df.empty:
                return 0
            
            conn_str = self.get_connection_string()
            with pyodbc.connect(conn_str) as conn:
                cursor = conn.cursor()
                
                # Import par batch de 1000
                batch_size = 1000
                records_imported = 0
                
                for i in range(0, len(df), batch_size):
                    batch = df.iloc[i:i+batch_size]
                    
                    # Pr√©parer les valeurs
                    values = []
                    for _, row in batch.iterrows():
                        values.append((
                            row['date_envoi'],
                            row['cle_bdf'], 
                            row['type_courrier']
                        ))
                    
                    # Insert batch
                    cursor.executemany("""
                        INSERT INTO InscriptionsFICP 
                        (DateInscription, CleBDF, TypeCourrier)
                        VALUES (?, ?, ?)
                    """, values)
                    
                    records_imported += len(batch)
                
                conn.commit()
                
                with self.lock:
                    self.total_records_imported += records_imported
                    self.total_files_processed += 1
                
                return records_imported
                
        except Exception as e:
            logger.error(f"‚ùå Erreur import inscriptions {file_path}: {e}")
            return 0
    
    def import_radiations_file(self, file_path):
        """Import d'un fichier de radiations vers Azure SQL"""
        try:
            df = pd.read_csv(file_path)
            if df.empty:
                return 0
            
            conn_str = self.get_connection_string()
            with pyodbc.connect(conn_str) as conn:
                cursor = conn.cursor()
                
                # Import par batch de 1000
                batch_size = 1000
                records_imported = 0
                
                for i in range(0, len(df), batch_size):
                    batch = df.iloc[i:i+batch_size]
                    
                    # Pr√©parer les valeurs
                    values = []
                    for _, row in batch.iterrows():
                        values.append((
                            row['date_radiation'],
                            row['cle_bdf'],
                            row['type_radiation'],
                            row['date_inscription_origine'],
                            row['duree_inscription_jours'],
                            row['montant_radie'],
                            row['organisme_demandeur'],
                            row['motif_detaille']
                        ))
                    
                    # Insert batch
                    cursor.executemany("""
                        INSERT INTO RadiationsFICP 
                        (DateRadiation, CleBDF, TypeRadiation, DateInscriptionOrigine, 
                         DureeInscriptionJours, MontantRadie, OrganismeDemandeur, MotifDetaille)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, values)
                    
                    records_imported += len(batch)
                
                conn.commit()
                
                with self.lock:
                    self.total_records_imported += records_imported
                    self.total_files_processed += 1
                
                return records_imported
                
        except Exception as e:
            logger.error(f"‚ùå Erreur import radiations {file_path}: {e}")
            return 0
    
    def get_all_files_by_type(self, base_path):
        """R√©cup√®re tous les fichiers par type"""
        files = {
            'consultations': [],
            'inscriptions': [],
            'radiations': []
        }
        
        base_dir = Path(base_path)
        
        # Parcourir tous les dossiers mois
        for type_dir in ['consultations', 'inscriptions', 'radiations']:
            type_path = base_dir / type_dir
            
            if type_path.exists():
                # Parcourir tous les dossiers mois (2025-01, 2025-02, etc.)
                for month_dir in type_path.iterdir():
                    if month_dir.is_dir():
                        # Tous les fichiers CSV du mois
                        for csv_file in month_dir.glob("*.csv"):
                            files[type_dir].append(csv_file)
        
        return files
    
    def import_files_parallel(self, files_list, import_function, type_name):
        """Import de fichiers en parall√®le"""
        logger.info(f"üöÄ D√©but import parall√®le {type_name} - {len(files_list)} fichiers")
        
        total_records = 0
        success_count = 0
        
        # ThreadPoolExecutor pour parall√©lisation
        with ThreadPoolExecutor(max_workers=5) as executor:
            # Soumettre tous les jobs
            future_to_file = {
                executor.submit(import_function, file_path): file_path 
                for file_path in files_list
            }
            
            # Traiter les r√©sultats
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    records = future.result()
                    total_records += records
                    success_count += 1
                    
                    if success_count % 10 == 0:
                        logger.info(f"  üìä {type_name}: {success_count}/{len(files_list)} fichiers trait√©s")
                        
                except Exception as e:
                    logger.error(f"‚ùå Erreur traitement {file_path}: {e}")
        
        logger.info(f"‚úÖ {type_name} termin√©: {total_records:,} enregistrements import√©s")
        return total_records
    
    def cramer_historique_complet(self):
        """CRAMAGE MASSIF DE L'HISTORIQUE COMPLET !"""
        logger.info("üî•üî•üî• D√âBUT CRAMAGE MASSIF AZURE SQL DATABASE ! üî•üî•üî•")
        logger.info("="*90)
        
        self.start_time = datetime.now()
        
        # R√©cup√©rer tous les fichiers
        base_path = "DataLakeE7/historique_quotidien"
        files = self.get_all_files_by_type(base_path)
        
        total_files = sum(len(file_list) for file_list in files.values())
        logger.info(f"üìÅ Fichiers trouv√©s:")
        logger.info(f"  üìã Consultations: {len(files['consultations'])}")
        logger.info(f"  üìù Inscriptions: {len(files['inscriptions'])}")
        logger.info(f"  ‚ò¢Ô∏è Radiations: {len(files['radiations'])}")
        logger.info(f"  üìä TOTAL: {total_files}")
        logger.info("="*90)
        
        if total_files == 0:
            logger.error("‚ùå Aucun fichier trouv√© !")
            return
        
        # Import s√©quentiel par type pour √©viter les locks Azure
        logger.info("üöÄ PHASE 1: Import des CONSULTATIONS")
        consultations_imported = self.import_files_parallel(
            files['consultations'], 
            self.import_consultations_file, 
            "CONSULTATIONS"
        )
        
        logger.info("üöÄ PHASE 2: Import des INSCRIPTIONS")
        inscriptions_imported = self.import_files_parallel(
            files['inscriptions'], 
            self.import_inscriptions_file, 
            "INSCRIPTIONS"
        )
        
        logger.info("üöÄ PHASE 3: Import des RADIATIONS")
        radiations_imported = self.import_files_parallel(
            files['radiations'], 
            self.import_radiations_file, 
            "RADIATIONS"
        )
        
        # Statistiques finales
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        logger.info("="*90)
        logger.info("üéâ CRAMAGE HISTORIQUE TERMIN√â !")
        logger.info("="*90)
        logger.info(f"üìã Consultations import√©es: {consultations_imported:,}")
        logger.info(f"üìù Inscriptions import√©es: {inscriptions_imported:,}")
        logger.info(f"‚ò¢Ô∏è Radiations import√©es: {radiations_imported:,}")
        logger.info(f"üìä TOTAL RECORDS: {consultations_imported + inscriptions_imported + radiations_imported:,}")
        logger.info(f"üìÅ Fichiers trait√©s: {self.total_files_processed}")
        logger.info(f"‚è±Ô∏è Dur√©e: {duration}")
        logger.info(f"üöÄ Throughput: {(consultations_imported + inscriptions_imported + radiations_imported) / duration.total_seconds():.0f} records/sec")
        logger.info("üí∞ CR√âDITS AZURE INTENSIVEMENT UTILIS√âS !")
        logger.info("="*90)

def main():
    """Fonction principale - Import massif Azure SQL"""
    print("üî•üî•üî• IMPORTEUR MASSIF AZURE SQL - CRAMAGE HISTORIQUE ! üî•üî•üî•")
    print("="*80)
    print("‚ö†Ô∏è ATTENTION: Import de 264,451+ enregistrements vers Azure SQL")
    print("üí∞ Utilisation MASSIVE des cr√©dits Azure gratuits")
    print("üöÄ Parall√©lisation maximale pour performance")
    print("="*80)
    
    confirmation = input("üöÄ Confirmer le CRAMAGE MASSIF ? (OUI pour continuer): ")
    if confirmation.upper() != 'OUI':
        print("‚ùå Import annul√©")
        return
    
    # Import massif
    importeur = ImporteurMassifAzureHistorique()
    
    try:
        importeur.cramer_historique_complet()
        print(f"\nüéä CRAMAGE R√âUSSI !")
        print(f"üí∞ Cr√©dits Azure utilis√©s de mani√®re INTENSIVE !")
        print(f"üìä Base de donn√©es pr√™te pour la certification E7 !")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cramage: {e}")
        print(f"‚ùå Erreur: {e}")

if __name__ == "__main__":
    main()