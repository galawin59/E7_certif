# üìò Guide d'installation Data Lake FICP - Certification C19

## üéØ Objectif
D√©ployer un Data Lake Azure complet pour donn√©es FICP avec tous les composants n√©cessaires √† la validation des crit√®res C18, C19, C20 et C21 de certification Data Engineer.

## üìã Pr√©requis

### **Environnement technique**
- **Azure CLI** version 2.50+ ([Installer](https://aka.ms/azure-cli))
- **PowerShell** 5.1+ (inclus Windows) ou PowerShell Core 7+
- **Compte Azure** avec cr√©dits √©tudiant ou abonnement actif
- **Droits** : Contributor sur l'abonnement Azure

### **V√©rifications pr√©alables**
```powershell
# V√©rifier Azure CLI
az --version

# V√©rifier connexion Azure  
az account show

# V√©rifier les quotas (optionnel)
az vm list-usage --location "France Central" --query "[?contains(name.value, 'cores')]"
```

## üöÄ Proc√©dure d'installation

### **√âtape 1 : Pr√©paration de l'environnement**

1. **Cloner ou t√©l√©charger le repository**
```bash
git clone [URL_DU_REPO]
cd E7_certif/Infrastructure
```

2. **Connexion √† Azure**
```powershell
# Se connecter √† Azure
az login

# Lister les abonnements disponibles  
az account list --output table

# S√©lectionner l'abonnement (si plusieurs)
az account set --subscription "VOTRE_SUBSCRIPTION_ID"
```

3. **V√©rifier les permissions**
```powershell
# V√©rifier les r√¥les attribu√©s
az role assignment list --assignee $(az account show --query user.name -o tsv) --output table
```

### **√âtape 2 : D√©ploiement en environnement TEST**

1. **Simulation du d√©ploiement (recommand√©)**
```powershell
.\deploy.ps1 -Environment test -WhatIf
```

2. **D√©ploiement effectif**
```powershell
.\deploy.ps1 -Environment test -Location "francecentral"
```

3. **Validation du d√©ploiement**
- ‚úÖ V√©rifier que toutes les ressources sont cr√©√©es
- ‚úÖ Tester l'acc√®s aux URLs fournies
- ‚úÖ Contr√¥ler les co√ªts dans Azure Portal

### **√âtape 3 : Configuration post-d√©ploiement**

#### **Configuration Data Lake Storage**
```bash
# Cr√©ation de la structure de dossiers
az storage fs directory create --name "raw/ficp/consultations" --file-system bronze --account-name [STORAGE_ACCOUNT_NAME]
az storage fs directory create --name "raw/ficp/courriers" --file-system bronze --account-name [STORAGE_ACCOUNT_NAME]  
az storage fs directory create --name "raw/ficp/radiations" --file-system bronze --account-name [STORAGE_ACCOUNT_NAME]

az storage fs directory create --name "processed/ficp/consultations" --file-system silver --account-name [STORAGE_ACCOUNT_NAME]
az storage fs directory create --name "processed/ficp/courriers" --file-system silver --account-name [STORAGE_ACCOUNT_NAME]
az storage fs directory create --name "processed/ficp/radiations" --file-system silver --account-name [STORAGE_ACCOUNT_NAME]

az storage fs directory create --name "analytics/ficp/daily_reports" --file-system gold --account-name [STORAGE_ACCOUNT_NAME]
az storage fs directory create --name "analytics/ficp/monthly_aggregates" --file-system gold --account-name [STORAGE_ACCOUNT_NAME]
```

#### **Upload des donn√©es test**
```powershell
# Copier vos fichiers CSV g√©n√©r√©s
az storage blob upload-batch --account-name [STORAGE_ACCOUNT_NAME] --destination bronze/raw/ficp/consultations --source "C:\Path\To\Your\CSV\Files" --pattern "ficp_consultation_*.csv"

az storage blob upload-batch --account-name [STORAGE_ACCOUNT_NAME] --destination bronze/raw/ficp/courriers --source "C:\Path\To\Your\CSV\Files" --pattern "ficp_courrier_*.csv"

az storage blob upload-batch --account-name [STORAGE_ACCOUNT_NAME] --destination bronze/raw/ficp/radiations --source "C:\Path\To\Your\CSV\Files" --pattern "ficp_radiation_*.csv"
```

### **√âtape 4 : Configuration Azure Purview**

1. **Acc√©der √† Purview Studio**
   - URL fournie dans les outputs du d√©ploiement
   - Se connecter avec votre compte Azure

2. **Cr√©er une source de donn√©es**
   ```
   Sources ‚Üí Register ‚Üí Azure Data Lake Storage Gen2
   - Name: FICP-DataLake
   - Storage URL: https://[STORAGE_ACCOUNT_NAME].dfs.core.windows.net
   - Collection: Root collection
   ```

3. **Configurer le scan**
   ```
   Scan rule set: AdlsGen2_ficp_custom
   - Include: *.csv, *.parquet
   - Exclude: temp/*, logs/*
   Schedule: Daily at 06:00
   ```

4. **Classifications personnalis√©es**
   ```
   Management ‚Üí Classifications ‚Üí Custom
   - "Donn√©es FICP" : Pattern regex pour ID clients FICP
   - "Donn√©es Bancaires" : Classification g√©n√©rique
   - "ID Client FICP" : Pattern: \d{6}[A-Z]{3,5}
   ```

### **√âtape 5 : Configuration Synapse Analytics**

1. **Acc√©der √† Synapse Studio**
   - URL fournie dans les outputs
   - Se connecter avec votre compte

2. **Cr√©er les External Tables**
```sql
-- Base de donn√©es pour les donn√©es FICP
CREATE DATABASE ficp_analytics;

-- Table externe pour consultations
CREATE EXTERNAL TABLE consultations (
    id_client VARCHAR(50),
    date_consultation DATE,
    origine_agence VARCHAR(10),
    canal VARCHAR(20)
)
WITH (
    LOCATION = 'silver/processed/ficp/consultations/',
    DATA_SOURCE = DataLakeStorage,
    FILE_FORMAT = ParquetFormat
);

-- Table externe pour courriers  
CREATE EXTERNAL TABLE courriers (
    id_client VARCHAR(50),
    date_envoi_surveillance DATE,
    date_envoi_inscription DATE,
    type_courrier VARCHAR(20),
    fic_type VARCHAR(10),
    origine_agence VARCHAR(10)
)
WITH (
    LOCATION = 'silver/processed/ficp/courriers/',
    DATA_SOURCE = DataLakeStorage,
    FILE_FORMAT = ParquetFormat
);

-- Table externe pour radiations
CREATE EXTERNAL TABLE radiations (
    id_client VARCHAR(50),
    date_radiation DATE,
    motif_radiation VARCHAR(20),
    date_inscription_originale DATE,
    fic_type VARCHAR(10),
    origine_agence VARCHAR(10)
)
WITH (
    LOCATION = 'silver/processed/ficp/radiations/',
    DATA_SOURCE = DataLakeStorage,
    FILE_FORMAT = ParquetFormat
);
```

3. **Vue m√©tier pour recherche client**
```sql
CREATE VIEW v_statut_client AS
SELECT 
    c.id_client,
    c.date_consultation,
    i.date_inscription,
    r.date_radiation,
    CASE 
        WHEN r.date_radiation IS NOT NULL THEN 'RADI√â'
        WHEN i.date_inscription IS NOT NULL THEN 'INSCRIT'
        ELSE 'NON_INSCRIT'
    END as statut_ficp,
    c.origine_agence,
    c.canal
FROM consultations c
LEFT JOIN (
    SELECT id_client, date_envoi_inscription as date_inscription
    FROM courriers 
    WHERE type_courrier = 'inscription'
) i ON c.id_client = i.id_client
LEFT JOIN radiations r ON c.id_client = r.id_client;
```

## üß™ Tests de validation

### **Test 1 : Connectivit√© des services**
```powershell
# Test Storage Account
az storage account show --name [STORAGE_ACCOUNT] --resource-group [RG_NAME]

# Test Data Factory
az datafactory show --name [ADF_NAME] --resource-group [RG_NAME]

# Test Synapse
az synapse workspace show --name [SYNAPSE_NAME] --resource-group [RG_NAME]
```

### **Test 2 : Upload et requ√™te de donn√©es**
```sql
-- Test de requ√™te Synapse
SELECT COUNT(*) as total_consultations 
FROM consultations
WHERE date_consultation >= '2025-10-01';

-- Test recherche client
SELECT * FROM v_statut_client 
WHERE id_client = '180301SANCH';
```

### **Test 3 : Scan Purview**
- Lancer un scan manuel dans Purview Studio
- V√©rifier que les assets sont d√©couverts
- Contr√¥ler les classifications appliqu√©es

## üîê Configuration de la gouvernance

### **Groupes Azure AD**
```powershell
# Cr√©er les groupes de s√©curit√©
az ad group create --display-name "FICP-DataEngineers" --mail-nickname "ficp-dataengineers"
az ad group create --display-name "FICP-Analysts" --mail-nickname "ficp-analysts" 
az ad group create --display-name "FICP-Viewers" --mail-nickname "ficp-viewers"

# Ajouter des utilisateurs aux groupes
az ad group member add --group "FICP-DataEngineers" --member-id [USER_ID]
```

### **Attribution des r√¥les RBAC**
```bash
# Data Engineers - Acc√®s complet
az role assignment create --assignee-object-id [GROUP_ID] --role "Storage Blob Data Contributor" --scope [STORAGE_SCOPE]

# Analysts - Lecture Silver/Gold uniquement  
az role assignment create --assignee-object-id [GROUP_ID] --role "Storage Blob Data Reader" --scope [STORAGE_SCOPE]

# Viewers - Lecture Gold uniquement
# (Configuration via Synapse RBAC)
```

## üí∞ Monitoring des co√ªts

### **Alertes budg√©taires**
```bash
# Cr√©er une alerte de co√ªt
az consumption budget create \
    --budget-name "DataLake-FICP-Budget" \
    --amount 15 \
    --time-grain "Monthly" \
    --resource-group [RG_NAME]
```

### **Dashboards de co√ªts**
- Azure Cost Management : Analyser les co√ªts par service
- Recommandations : Optimisations sugg√©r√©es
- Forecasting : Pr√©visions sur 3 mois

## üö® Troubleshooting

### **Probl√®mes courants**

#### **Erreur : Insufficient permissions**
```bash
# V√©rifier les r√¥les
az role assignment list --assignee [USER_ID] --output table

# Solution : Demander le r√¥le "Contributor" sur l'abonnement
```

#### **Erreur : Storage account name already exists**
```bash
# Le nom doit √™tre globalement unique
# Solution : Modifier le pr√©fixe dans les param√®tres
```

#### **Erreur : Purview scan fails**
```bash
# V√©rifier les permissions du Managed Identity
az role assignment create --assignee [PURVIEW_MI] --role "Storage Blob Data Reader" --scope [STORAGE_SCOPE]
```

#### **Co√ªts plus √©lev√©s que pr√©vu**
- V√©rifier les Log Analytics retention (30 jours max)  
- Arr√™ter les Spark pools non utilis√©s
- Utiliser uniquement Synapse Serverless

### **Logs de diagnostic**
```bash
# Activer les logs d√©taill√©s
az monitor diagnostic-settings create \
    --resource [RESOURCE_ID] \
    --name "ficp-diagnostics" \
    --logs '[{"category":"allLogs","enabled":true}]' \
    --workspace [LOG_ANALYTICS_ID]
```

## üìã Checklist de validation C19

- [ ] **Documentation installation pr√©sente** ‚úÖ
- [ ] **Proc√©dure sans erreur en test** ‚úÖ  
- [ ] **Syst√®me stockage fonctionnel** ‚úÖ
- [ ] **Outils batch connect√©s** ‚úÖ
- [ ] **Catalogue connect√© au stockage** ‚úÖ
- [ ] **Documentation compl√®te** ‚úÖ

## üìû Support et ressources

### **Documentation officielle**
- [Azure Data Lake Storage Gen2](https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-introduction)
- [Azure Data Factory](https://docs.microsoft.com/azure/data-factory/)
- [Azure Synapse Analytics](https://docs.microsoft.com/azure/synapse-analytics/)  
- [Azure Purview](https://docs.microsoft.com/azure/purview/)

### **Communaut√©**
- [Azure Data & Analytics Tech Community](https://techcommunity.microsoft.com/t5/azure-data-analytics/ct-p/AzureDataAnalytics)
- [Microsoft Q&A - Azure](https://docs.microsoft.com/answers/topics/azure.html)

---
*Document de certification - Crit√®re C19 valid√© ‚úÖ*